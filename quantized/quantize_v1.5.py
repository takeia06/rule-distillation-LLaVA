import torch
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer
from datasets import load_dataset  # <-- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è¿½åŠ 

# ãƒ¢ãƒ‡ãƒ«ã¨ä¿å­˜å…ˆã®ãƒ‘ã‚¹a
model_path = 'llava-hf/llava-1.5-7b-hf'
quant_path = 'llava-v1.5-7b-awq-vllm'

# é‡å­åŒ–ã®è¨­å®š
quant_config = {
    "w_bit": 4,
    "q_group_size": 128,
    "zero_point": True,
    "version": "GEMM"
}

# --- ã€æœ€é‡è¦ã€‘ã“ã“ãŒçŸ­ã„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹éƒ¨åˆ†ã§ã™ ---
print("ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã€çŸ­ã„ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ‰‹å‹•ã§æº–å‚™ã—ã¾ã™...")
calib_dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
calib_data = [d["text"] for d in calib_dataset if len(d["text"]) > 10 and len(d["text"]) < 512][:32]
print(f"{len(calib_data)}å€‹ã®çŸ­ã„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã—ãŸã€‚")
# --- ã“ã“ã¾ã§ ---

print(f"'{model_path}' ã®é‡å­åŒ–ã‚’é–‹å§‹ã—ã¾ã™...")
model = AutoAWQForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    safetensors=True,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True, 
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

print("\nãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ã“ã®å‡¦ç†ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...")
# æ‰‹å‹•ã§æº–å‚™ã—ãŸã€ŒçŸ­ã„ã€ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™
model.quantize(
    tokenizer,
    quant_config=quant_config,
    calib_data=calib_data
)

print(f"\né‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ '{quant_path}' ã«ä¿å­˜ã—ã¾ã™...")
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)

print(f"\nðŸŽ‰ é‡å­åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼'{quant_path}' ã«ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")