import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# --- è¨­å®šé …ç›® ---
# LoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
peft_model_path = '/home/takei/LLaVA/checkpoints/llava-v1.6-vicuna-7b-mvtec/checkpoint-907' 
# ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆãƒ‘ã‚¹
merged_model_path = 'checkpoints/llava-v1.6-vicuna-7b-mvtec-merged'

print(f"'{peft_model_path}' ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
base_model = AutoModelForCausalLM.from_pretrained(
    "lmsys/vicuna-7b-v1.5", # ã“ã“ã¯ã”è‡ªèº«ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦ãã ã•ã„
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto"
)

# PEFTãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã™ã‚‹
model = PeftModel.from_pretrained(
    base_model,
    peft_model_path
)

print("LoRAãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ãƒãƒ¼ã‚¸ã—ã¦ã„ã¾ã™...")
# LoRAãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ãƒãƒ¼ã‚¸ã—ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™
model = model.merge_and_unload()
print("ãƒãƒ¼ã‚¸ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

print(f"ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ '{merged_model_path}' ã«ä¿å­˜ã—ã¾ã™...")
model.save_pretrained(merged_model_path)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚‚ã‚³ãƒ”ãƒ¼ã™ã‚‹
tokenizer = AutoTokenizer.from_pretrained(peft_model_path)
tokenizer.save_pretrained(merged_model_path)

print(f"\nğŸ‰ ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼'{merged_model_path}' ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")