import os
import torch
from transformers import LlavaNextForConditionalGeneration, AutoTokenizer, LlavaNextProcessor
from peft import PeftModel
from awq import AutoAWQForCausalLM
from datasets import load_dataset  # ã‚ãªãŸæ§˜ã®æˆåŠŸã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¡ç”¨

def merge_lora(base_model_id, lora_weights_path, merged_model_path):
    """
    ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«LoRAã®é‡ã¿ã‚’ãƒãƒ¼ã‚¸ã—ã¦ä¿å­˜ã™ã‚‹é–¢æ•°
    """
    print("--- LoRAã®ãƒãƒ¼ã‚¸ã‚’é–‹å§‹ã—ã¾ã™ ---")
    # ãƒãƒ¼ã‚¸å‡¦ç†ã¯CPUã§è¡Œã„ã€å¾Œç¶šã®é‡å­åŒ–ã®ãŸã‚ã«GPUãƒ¡ãƒ¢ãƒªã‚’æ¸©å­˜ã—ã¾ã™
    base_model = LlavaNextForConditionalGeneration.from_pretrained(
        base_model_id,
        torch_dtype="auto",
        device_map="cpu"
    )
    model = PeftModel.from_pretrained(base_model, lora_weights_path)
    model = model.merge_and_unload()
    model.save_pretrained(merged_model_path)
    print(f"LoRAçµ±åˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ {merged_model_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return merged_model_path

def quantize_awq(merged_model_path, quantized_model_path):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’AWQã§é‡å­åŒ–ã™ã‚‹é–¢æ•°
    """
    print("--- AWQé‡å­åŒ–ã‚’é–‹å§‹ã—ã¾ã™ ---")
    
    # é‡å­åŒ–è¨­å®š
    quant_config = {
        "w_bit": 4,
        "q_group_size": 128,
        "zero_point": True,
        "version": "GEMM"
    }

    # ã€ã“ã‚ŒãŒè§£æ±ºç­–ã§ã™ã€‘å‹•ä½œã—ãŸã‚³ãƒ¼ãƒ‰ã‚’å…ƒã«ã€é«˜å“è³ªãªçŸ­ã„ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    print("é«˜å“è³ªãªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™...")
    calib_dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
    # 10æ–‡å­—ä»¥ä¸Š512æ–‡å­—æœªæº€ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’32å€‹ä½¿ç”¨ã—ã¾ã™
    calib_data = [d["text"] for d in calib_dataset if d["text"] and 10 < len(d["text"]) < 512][:32]
    print(f"{len(calib_data)}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã—ãŸã€‚")

    # å‹•ä½œã—ãŸã‚³ãƒ¼ãƒ‰ã‚’å‚è€ƒã«ã€åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
    print(f"ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« '{merged_model_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
    model = AutoAWQForCausalLM.from_pretrained(
        merged_model_path,
        trust_remote_code=True,
        safetensors=True,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="auto"  # GPUã«è‡ªå‹•ã§é…ç½®
    )

    tokenizer = AutoTokenizer.from_pretrained(merged_model_path, trust_remote_code=True)

    # æ‰‹å‹•ã§æº–å‚™ã—ãŸã€ŒçŸ­ã„ã€ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦é‡å­åŒ–ã—ã¾ã™
    print("\nãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ã“ã®å‡¦ç†ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        calib_data=calib_data  # max_seq_lenã¯ä½¿ã‚ãšã€ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥æ¸¡ã—ã¾ã™
    )

    # é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    print(f"\né‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ '{quantized_model_path}' ã«ä¿å­˜ã—ã¾ã™...")
    model.save_quantized(quantized_model_path)
    tokenizer.save_pretrained(quantized_model_path)
    
    print(f"\nğŸ‰ æœ€çµ‚çš„ãªé‡å­åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼'{quantized_model_path}' ã«ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")

def copy_tokenizer_and_processor(base_model_id, save_dir):
    """
    ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®è¨­å®šã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹é–¢æ•°
    """
    print("--- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã™ ---")
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model_id)
        tokenizer.save_pretrained(save_dir)
    except Exception as e:
        print(f"Tokenizerã®ã‚³ãƒ”ãƒ¼ã«å¤±æ•—: {e}")

    try:
        processor = LlavaNextProcessor.from_pretrained(base_model_id)
        processor.save_pretrained(save_dir)
    except Exception as e:
        print(f"Processorã®ã‚³ãƒ”ãƒ¼ã«å¤±æ•—: {e}")

if __name__ == "__main__":
    # --- è¨­å®šé …ç›® ---
    base_model_id = "llava-v1.6-vicuna-7b-hf"
    lora_weights_path = "/home/takei/LLaVA/checkpoints/llava-v1.6-vicuna-7b-mvtec/checkpoint-907"
    merged_model_path = "merged_model"
    quantized_model_path = "checkpoints/llava-v1.6-vicuna-7b-mvtec-awq"
    
    # --- å®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹ ---
    # 1. LoRAå±¤ã‚’ãƒãƒ¼ã‚¸
    merge_lora(base_model_id, lora_weights_path, merged_model_path)
    
    # 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®è¨­å®šã‚’ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
    copy_tokenizer_and_processor(base_model_id, merged_model_path)
    
    # 3. AWQã§é‡å­åŒ–
    quantize_awq(merged_model_path, quantized_model_path)