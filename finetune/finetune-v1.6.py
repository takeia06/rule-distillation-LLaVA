import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from torch.cuda.amp import GradScaler

from transformers import (
    AutoProcessor,
    AutoConfig,
    LlavaNextForConditionalGeneration,
    BitsAndBytesConfig,
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

from PIL import Image
import os
import csv
from tqdm import tqdm
import time
import gc ### å¤‰æ›´ç‚¹: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ###

# --- 1. è¨­å®šé …ç›® ---
# (å¤‰æ›´ãªã—)
# --- ãƒ¢ãƒ‡ãƒ«è¨­å®š ---
TEACHER_MODEL_ID = "llava-hf/llava-v1.6-vicuna-7b-hf"
STUDENT_MODEL_ID = "llava-hf/llava-v1.6-vicuna-7b-hf"

# --- ãƒ‡ãƒ¼ã‚¿è¨­å®š ---
TRAIN_CSV_PATH = "data/VisA/chewinggum/full/train.csv"
EVAL_CSV_PATH = "data/VisA/chewinggum/no/val.csv"

# --- å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
OUTPUT_DIR = "student_model_v1.6_output/"
NUM_EPOCHS = 20
BATCH_SIZE = 1
GRAD_ACCUMULATION_STEPS = 8
LEARNING_RATE = 2e-5
MAX_NEW_TOKENS_TEACHER = 512

# --- æå¤±ã®é‡ã¿è¨­å®š ---
LOSS_WEIGHT_RESPONSE = 1.0
LOSS_WEIGHT_LOGITS = 0.5
LOSS_WEIGHT_HIDDEN = 0.2

# --- LoRAè¨­å®š ---
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
LORA_TARGET_MODULES = ["q_proj", "v_proj", "k_proj", "o_proj"]
# -------------------------------------------------------------

# (load_models_and_processors, OnlineDistillationDataset, custom_collate_fn, evaluate é–¢æ•°ã¯å¤‰æ›´ãªã—)
def load_models_and_processors():
    """æ•™å¸«ãƒ»ç”Ÿå¾’ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""
    print("1. æ•™å¸«ãƒ»ç”Ÿå¾’ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãã‚Œãã‚Œãƒ­ãƒ¼ãƒ‰ä¸­...")
    teacher_processor = AutoProcessor.from_pretrained(TEACHER_MODEL_ID)
    student_processor = AutoProcessor.from_pretrained(STUDENT_MODEL_ID)
    print("   >>> ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")

    print("2. ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆå›³(config)ã‚’æº–å‚™ä¸­...")
    model_config = AutoConfig.from_pretrained(TEACHER_MODEL_ID)
    try:
        image_grid_pinpoints = teacher_processor.image_processor.image_grid_pinpoints
        model_config.image_grid_pinpoints = image_grid_pinpoints
        print(f"   >>> å–å¾—ã—ãŸã‚°ãƒªãƒƒãƒ‰è¨­å®š: {image_grid_pinpoints}")
    except AttributeError:
        print("   >>> è­¦å‘Š: ãƒ—ãƒ­ã‚»ãƒƒã‚µã‹ã‚‰ image_grid_pinpoints ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    print("   >>> è¨­è¨ˆå›³(config)ã®æº–å‚™å®Œäº†ï¼")
    
    print("3. æœ€æ–°ã®8-bité‡å­åŒ–è¨­å®šã‚’æº–å‚™ä¸­...")
    quantization_config = BitsAndBytesConfig(load_in_8bit=True)
    print("   >>> é‡å­åŒ–è¨­å®šã®æº–å‚™å®Œäº†ï¼")

    print(f"4. æ•™å¸«ãƒ¢ãƒ‡ãƒ« '{TEACHER_MODEL_ID}' (8-bit) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    teacher_model = LlavaNextForConditionalGeneration.from_pretrained(
        TEACHER_MODEL_ID,
        config=model_config,
        quantization_config=quantization_config,
        torch_dtype=torch.float16,
    ).eval()
    print("   >>> æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")

    print(f"5. ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ« '{STUDENT_MODEL_ID}' (8-bit LoRA) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    student_model = LlavaNextForConditionalGeneration.from_pretrained(
        STUDENT_MODEL_ID,
        config=model_config,
        quantization_config=quantization_config,
        torch_dtype=torch.float16,
    )
    student_model = prepare_model_for_kbit_training(student_model)
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=LORA_TARGET_MODULES,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
    )
    student_model = get_peft_model(student_model, lora_config)
    student_model.print_trainable_parameters()
    print("   >>> ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨LoRAã®é©ç”¨å®Œäº†ï¼\n")

    return teacher_model, student_model, teacher_processor, student_processor

class OnlineDistillationDataset(Dataset):
    """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è’¸ç•™ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    def __init__(self, csv_path, teacher_processor, student_processor):
        self.teacher_processor = teacher_processor
        self.student_processor = student_processor
        try:
            with open(csv_path, 'r', encoding='utf-8') as f:
                self.tasks = list(csv.DictReader(f))
            print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {csv_path} ã‹ã‚‰ {len(self.tasks)} ä»¶ã®ã‚¿ã‚¹ã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
        except FileNotFoundError:
            print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« '{csv_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            self.tasks = []

    def __len__(self):
        return len(self.tasks)

    def __getitem__(self, idx):
        if not self.tasks: return None
        task = self.tasks[idx]
        image_path = task['image_path']
        instruction = task['instruction']

        try:
            image = Image.open(image_path).convert('RGB')
        except FileNotFoundError:
            print(f"è­¦å‘Š: ç”»åƒ '{image_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¿ã‚¹ã‚¯ {idx} ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return None

        vicuna_system_prompt = (
            "A chat between a curious user and an artificial intelligence assistant. "
            "The assistant gives helpful, detailed, and polite answers to the user's questions."
        )
        
        teacher_prompt = (
            f"{vicuna_system_prompt} "
            f"USER: <image>\n{instruction}\n"
            "ASSISTANT:"
        )

        student_prompt = (
            f"{vicuna_system_prompt} "
            f"USER: <image>\n"
            "ASSISTANT:"
        )
        
        teacher_inputs = self.teacher_processor(
            text=teacher_prompt, images=image, return_tensors="pt"
        )
        student_inputs = self.student_processor(
            text=student_prompt, images=image, return_tensors="pt"
        )

        return {
            "teacher_pixel_values": teacher_inputs.pixel_values.squeeze(0),
            "student_pixel_values": student_inputs.pixel_values.squeeze(0),
            "image_sizes": teacher_inputs.get("image_sizes", [image.size]).squeeze(0),
            "teacher_input_ids": teacher_inputs.input_ids.squeeze(0),
            "teacher_attention_mask": teacher_inputs.attention_mask.squeeze(0),
            "student_input_ids": student_inputs.input_ids.squeeze(0),
            "student_attention_mask": student_inputs.attention_mask.squeeze(0),
        }

def custom_collate_fn(batch):
    """Noneã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ã«ãƒãƒƒãƒåŒ–ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ collateé–¢æ•°"""
    batch = [item for item in batch if item is not None]
    if not batch:
        return {}
    from torch.utils.data.dataloader import default_collate
    return default_collate(batch)


def evaluate(student_model, teacher_model, dataloader, device):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’è¡Œã†é–¢æ•°"""
    student_model.eval()
    teacher_model.eval()
    total_eval_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            if not batch: continue

            for key, value in batch.items():
                if isinstance(value, torch.Tensor):
                    batch[key] = value.to(device)

            with autocast(device_type="cuda", dtype=torch.float16):
                # --- ã‚¹ãƒ†ãƒƒãƒ—A: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã§ãŠæ‰‹æœ¬ã‚’ç”Ÿæˆ ---
                teacher_inputs = {
                    "input_ids": batch["teacher_input_ids"],
                    "pixel_values": batch["teacher_pixel_values"],
                    "attention_mask": batch["teacher_attention_mask"],
                }
                if "image_sizes" in batch:
                    teacher_inputs["image_sizes"] = batch["image_sizes"]
                
                generated_ids = teacher_model.generate(
                    **teacher_inputs, max_new_tokens=MAX_NEW_TOKENS_TEACHER, do_sample=False
                )
                prompt_len = teacher_inputs["input_ids"].shape[1]
                teacher_answer_ids = generated_ids[:, prompt_len:]

                full_teacher_input_ids = torch.cat([teacher_inputs["input_ids"], teacher_answer_ids], dim=1)
                full_teacher_attention_mask = torch.ones_like(full_teacher_input_ids).to(device)

                forward_teacher_inputs = {
                    "input_ids": full_teacher_input_ids,
                    "attention_mask": full_teacher_attention_mask,
                    "pixel_values": batch["teacher_pixel_values"],
                    "output_hidden_states": True,
                }
                if "image_sizes" in batch:
                    forward_teacher_inputs["image_sizes"] = batch["image_sizes"]
                
                teacher_forward_outputs = teacher_model(**forward_teacher_inputs)
                
                # --- ã‚¹ãƒ†ãƒƒãƒ—B: ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ ---
                student_full_input_ids = torch.cat([batch["student_input_ids"], teacher_answer_ids], dim=1)
                student_full_attention_mask = torch.cat([batch["student_attention_mask"], torch.ones_like(teacher_answer_ids)], dim=1)
                labels = student_full_input_ids.clone()
                student_prompt_len = batch["student_input_ids"].shape[1]
                labels[:, :student_prompt_len] = -100

                student_inputs = {
                    "pixel_values": batch["student_pixel_values"],
                    "input_ids": student_full_input_ids,
                    "attention_mask": student_full_attention_mask,
                    "labels": labels,
                    "output_hidden_states": True,
                }
                if "image_sizes" in batch:
                    student_inputs["image_sizes"] = batch["image_sizes"]
                
                student_outputs = student_model(**student_inputs)

                # --- ã‚¹ãƒ†ãƒƒãƒ—C: æå¤±è¨ˆç®— ---
                loss_response = student_outputs.loss
                
                with autocast(device_type="cuda", enabled=False):
                    teacher_answer_start = prompt_len
                    student_answer_start = student_prompt_len
                    
                    teacher_logits_answer = teacher_forward_outputs.logits[:, teacher_answer_start:, :].float()
                    teacher_hidden_answer = teacher_forward_outputs.hidden_states[-1][:, teacher_answer_start:, :].float()
                    
                    student_logits_answer = student_outputs.logits[:, student_answer_start:, :].float()
                    student_hidden_answer = student_outputs.hidden_states[-1][:, student_answer_start:, :].float()
                    
                    seq_len = min(student_logits_answer.shape[1], teacher_logits_answer.shape[1])
                    
                    loss_logits = F.kl_div(
                        F.log_softmax(student_logits_answer[:, :seq_len, :], dim=-1),
                        F.softmax(teacher_logits_answer[:, :seq_len, :], dim=-1),
                        reduction='batchmean', log_target=False
                    )
                    loss_hidden = F.mse_loss(
                        student_hidden_answer[:, :seq_len, :], teacher_hidden_answer[:, :seq_len, :]
                    )
                    
                total_loss = (
                    LOSS_WEIGHT_RESPONSE * loss_response.float() +
                    LOSS_WEIGHT_LOGITS * loss_logits +
                    LOSS_WEIGHT_HIDDEN * loss_hidden
                )
                total_eval_loss += total_loss.item()

    avg_eval_loss = total_eval_loss / len(dataloader)
    return avg_eval_loss

def main():
    print("--- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è’¸ç•™ v2.3 (ãƒ¡ãƒ¢ãƒªè§£æ”¾å¯¾å¿œ) ---")

    teacher_model, student_model, teacher_processor, student_processor = load_models_and_processors()
    device = student_model.device

    print("\n--- ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ä¸­... ---")
    train_dataset = OnlineDistillationDataset(
        csv_path=TRAIN_CSV_PATH,
        teacher_processor=teacher_processor,
        student_processor=student_processor
    )
    train_dataloader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn
    )

    eval_dataset = OnlineDistillationDataset(
        csv_path=EVAL_CSV_PATH,
        teacher_processor=teacher_processor,
        student_processor=student_processor
    )
    eval_dataloader = DataLoader(
        eval_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn
    )

    if not train_dataset.tasks:
        print("ã‚¨ãƒ©ãƒ¼: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)
    total_training_steps = (len(train_dataloader) // GRAD_ACCUMULATION_STEPS) * NUM_EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=0, num_training_steps=total_training_steps
    )
    scaler = GradScaler()

    print("\n--- ğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ï¼ ---")
    global_step = 0
    for epoch in range(NUM_EPOCHS):
        print(f"\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---")
        
        student_model.train()
        total_train_loss = 0
        
        for step, batch in enumerate(tqdm(train_dataloader, desc=f"Training Epoch {epoch+1}")):
            if not batch: continue

            # (è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å†…éƒ¨ã¯å¤‰æ›´ãªã—)
            for key, value in batch.items():
                if isinstance(value, torch.Tensor):
                    batch[key] = value.to(device)

            with autocast(device_type="cuda", dtype=torch.float16):
                with torch.no_grad():
                    teacher_inputs = {
                        "input_ids": batch["teacher_input_ids"],
                        "pixel_values": batch["teacher_pixel_values"],
                        "attention_mask": batch["teacher_attention_mask"],
                    }
                    if "image_sizes" in batch:
                        teacher_inputs["image_sizes"] = batch["image_sizes"]
                    
                    generated_ids = teacher_model.generate(
                        **teacher_inputs, max_new_tokens=MAX_NEW_TOKENS_TEACHER, do_sample=False
                    )
                    prompt_len = teacher_inputs["input_ids"].shape[1]
                    teacher_answer_ids = generated_ids[:, prompt_len:]

                    full_teacher_input_ids = torch.cat([teacher_inputs["input_ids"], teacher_answer_ids], dim=1)
                    full_teacher_attention_mask = torch.ones_like(full_teacher_input_ids).to(device)

                    forward_teacher_inputs = {
                        "input_ids": full_teacher_input_ids,
                        "attention_mask": full_teacher_attention_mask,
                        "pixel_values": batch["teacher_pixel_values"],
                        "output_hidden_states": True,
                    }
                    if "image_sizes" in batch:
                        forward_teacher_inputs["image_sizes"] = batch["image_sizes"]
                    
                    teacher_forward_outputs = teacher_model(**forward_teacher_inputs)
                
                student_full_input_ids = torch.cat([batch["student_input_ids"], teacher_answer_ids], dim=1)
                student_full_attention_mask = torch.cat([batch["student_attention_mask"], torch.ones_like(teacher_answer_ids)], dim=1)
                labels = student_full_input_ids.clone()
                student_prompt_len = batch["student_input_ids"].shape[1]
                labels[:, :student_prompt_len] = -100

                student_inputs = {
                    "pixel_values": batch["student_pixel_values"],
                    "input_ids": student_full_input_ids,
                    "attention_mask": student_full_attention_mask,
                    "labels": labels,
                    "output_hidden_states": True,
                }
                if "image_sizes" in batch:
                    student_inputs["image_sizes"] = batch["image_sizes"]

                student_outputs = student_model(**student_inputs)

                loss_response = student_outputs.loss

                with autocast(device_type="cuda", enabled=False):
                    teacher_answer_start = prompt_len
                    student_answer_start = student_prompt_len
                    teacher_logits_answer = teacher_forward_outputs.logits[:, teacher_answer_start:, :].float()
                    teacher_hidden_answer = teacher_forward_outputs.hidden_states[-1][:, teacher_answer_start:, :].float()
                    student_logits_answer = student_outputs.logits[:, student_answer_start:, :].float()
                    student_hidden_answer = student_outputs.hidden_states[-1][:, student_answer_start:, :].float()
                    seq_len = min(student_logits_answer.shape[1], teacher_logits_answer.shape[1])
                    
                    loss_logits = F.kl_div(
                        F.log_softmax(student_logits_answer[:, :seq_len, :], dim=-1),
                        F.softmax(teacher_logits_answer[:, :seq_len, :], dim=-1),
                        reduction='batchmean', log_target=False
                    )
                    loss_hidden = F.mse_loss(
                        student_hidden_answer[:, :seq_len, :], teacher_hidden_answer[:, :seq_len, :]
                    )
                
                total_loss = (
                    LOSS_WEIGHT_RESPONSE * loss_response.float() +
                    LOSS_WEIGHT_LOGITS * loss_logits +
                    LOSS_WEIGHT_HIDDEN * loss_hidden
                )
            
            total_train_loss += total_loss.item()
            scaled_loss = scaler.scale(total_loss) / GRAD_ACCUMULATION_STEPS
            scaled_loss.backward()

            if (step + 1) % GRAD_ACCUMULATION_STEPS == 0 or (step + 1) == len(train_dataloader):
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()

                if global_step % 10 == 0:
                    print(f"Epoch: {epoch + 1}, Step: {global_step}, Train Loss: {total_loss.item():.4f}")
                global_step += 1
        
        avg_train_loss = total_train_loss / len(train_dataloader)
        print(f"--- Epoch {epoch + 1} è¨“ç·´å®Œäº† ---")
        print(f"Average Training Loss: {avg_train_loss:.4f}")
        
        ### å¤‰æ›´ç‚¹: ã“ã“ã‹ã‚‰ ###
        # è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚ºã®å‰ã«ã€ä¸è¦ã«ãªã£ãŸå¤‰æ•°ã‚’å‰Šé™¤ã—ã€CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹
        print("ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...")
        del batch, total_loss, scaled_loss, student_outputs, teacher_forward_outputs
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ¨ ãƒ¡ãƒ¢ãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†ï¼")
        ### å¤‰æ›´ç‚¹: ã“ã“ã¾ã§ ###

        if eval_dataloader and eval_dataset.tasks:
            print(f"--- Epoch {epoch + 1} è©•ä¾¡é–‹å§‹ ---")
            avg_eval_loss = evaluate(student_model, teacher_model, eval_dataloader, device)
            print(f"--- Epoch {epoch + 1} è©•ä¾¡å®Œäº† ---")
            print(f"âœ… Average Evaluation Loss: {avg_eval_loss:.4f}")
        else:
            print("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

        epoch_output_dir = os.path.join(OUTPUT_DIR, f"epoch_{epoch+1}")
        os.makedirs(epoch_output_dir, exist_ok=True)
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ {epoch_output_dir} ã«ä¿å­˜ä¸­...")
        student_model.save_pretrained(epoch_output_dir)

    print("\nğŸ‰ å…¨ã¦ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == '__main__':
    main()